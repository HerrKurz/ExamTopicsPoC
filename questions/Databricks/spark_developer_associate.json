[
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 85 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 8 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 65 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 79 discussion.html",
        "question": "Which of the following code blocks returns a 10 percent sample of rows from DataFrame storesDF with replacement?",
        "choices": [
            "A.storesDF.sample(true)",
            "B.storesDF.sample(true, fraction = 0.1)",
            "C.storesDF.sample(true, fraction = 0.15)",
            "D.storesDF.sampleBy(fraction = 0.1)",
            "E.storesDF.sample(false, fraction = 0.1)"
        ],
        "correct_answer": "B",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 119 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 114 discussion.html",
        "question": "Which of the following code blocks reads a CSV at the file path filePath into a Data Frame with the specified schema schema?",
        "choices": [
            "A.spark.read().csv(filePath)",
            "B.spark.read().schema(‚Äúschema‚Äù).csv(filePath)",
            "C.spark.read.schema(schema).csv(filePath)",
            "D.spark.read.schema(‚Äúschema‚Äù).csv(filePath)",
            "E.spark.read().schema(schema).csv(filePath)"
        ],
        "correct_answer": "C",
        "user_data": [
            {
                "voted_answers": "C",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 9 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 95 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 96 discussion.html",
        "question": "Which of the following code blocks writes DataFrame storesDF to file path filePath as parquet and partitions by values in column division?",
        "choices": [
            "A.storesDF.write.partitionBy(col(\"division\")).path(filePath)",
            "B.storesDF.write.option(\"parquet\").partitionBy(\"division\").path(filePath)",
            "C.storesDF.write.option(\"parquet\").partitionBy(col(\"division\")).path(filePath)",
            "D.storesDF.write.partitionBy(\"division\").parquet(filePath)",
            "E.storesDF.write().partitionBy(\"division\").parquet(filePath)"
        ],
        "correct_answer": "D",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 82 discussion.html",
        "question": "A data engineering team has noticed that their Databricks SQL queries are running too slowly when they are submitted to a non-running SQL endpoint. The data engineering team wants this issue to be resolved.Which of the following approaches can the team use to reduce the time it takes to return results in this scenario?",
        "choices": [
            "A.They can turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy to \"Reliability Optimized.\"",
            "B.They can turn on the Auto Stop feature for the SQL endpoint.",
            "C.They can increase the cluster size of the SQL endpoint.",
            "D.They can turn on the Serverless feature for the SQL endpoint.",
            "E.They can increase the maximum bound of the SQL endpoint's scaling range."
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "D",
                "vote_count": 17,
                "is_most_voted": true
            },
            {
                "voted_answers": "C",
                "vote_count": 5,
                "is_most_voted": true
            },
            {
                "voted_answers": "E",
                "vote_count": 2,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 42 discussion.html",
        "question": "The code block shown below contains an error. The code block is intended to create a Python UDF assessPerformanceUDF() using the integer-returning Python function assessPerformance() and apply it to column customerSatisfaction in DataFrame storesDF. Identify the error.Code block:assessPerformanceUDF ‚Äì udf(assessPerformance)storesDF.withColumn(\"result\", assessPerformanceUDF(col(\"customerSatisfaction\")))",
        "choices": [
            "A.The assessPerformance() operation is not properly registered as a UDF.",
            "B.The withColumn() operation is not appropriate here ‚Äì UDFs should be applied by iterating over rows instead.",
            "C.UDFs can only be applied vie SQL and not through the DataFrame API.",
            "D.The return type of the assessPerformanceUDF() is not specified in the udf() operation.",
            "E.The assessPerformance() operation should be used on column customerSatisfaction rather than the assessPerformanceUDF() operation."
        ],
        "correct_answer": "A",
        "user_data": [
            {
                "voted_answers": "D",
                "vote_count": 5,
                "is_most_voted": true
            },
            {
                "voted_answers": "A",
                "vote_count": 3,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 107 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 61 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 5 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 170 discussion.html",
        "question": "Which of the following operations can be used to rename and replace an existing column in a DataFrame?",
        "choices": [
            "A.DataFrame.renamedColumn()",
            "B.DataFrame.withColumnRenamed()",
            "C.DataFrame.wlthColumn()",
            "D.col()",
            "E.DataFrame.newColumn()"
        ],
        "correct_answer": "B",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 72 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 7 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 27 discussion.html",
        "question": "Which of the following code blocks returns a new DataFrame with column storeDescription where the pattern \"Description: \" has been removed from the beginning of column storeDescription in DataFrame storesDF?A sample of DataFrame storesDF is below:",
        "choices": [
            "A.storesDF.withColumn(\"storeDescription\", regexp_replace(col(\"storeDescription\"), \"^Description: \"))",
            "B.storesDF.withColumn(\"storeDescription\", col(\"storeDescription\").regexp_replace(\"^Description: \", \"\"))",
            "C.storesDF.withColumn(\"storeDescription\", regexp_extract(col(\"storeDescription\"), \"^Description: \", \"\"))",
            "D.storesDF.withColumn(\"storeDescription\", regexp_replace(\"storeDescription\", \"^Description: \", \"\"))",
            "E.storesDF.withColumn(\"storeDescription\", regexp_replace(col(\"storeDescription\"), \"^Description: \", \"\"))"
        ],
        "correct_answer": "E",
        "user_data": [
            {
                "voted_answers": "E",
                "vote_count": 9,
                "is_most_voted": true
            },
            {
                "voted_answers": "D",
                "vote_count": 5,
                "is_most_voted": false
            },
            {
                "voted_answers": "A",
                "vote_count": 2,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 87 discussion.html",
        "question": "The code block shown below should return a new 12-partition DataFrame from DataFrame storesDF. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.Code block:__1__.__2__(__3__)",
        "choices": [
            "A.1. storesDF2. coalesce3. 4",
            "B.1. storesDF2. coalesce3. 4, \"storeId\"",
            "C.1. storesDF2. repartition3. \"storeId\"",
            "D.1. storesDF2. repartition3. 12",
            "E.1. storesDF2. repartition3. Nothing"
        ],
        "correct_answer": "D",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 101 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 134 discussion.html",
        "question": "The code block shown below contains an error. The code block is intended to return a new DataFrame from DataFrame storesDF where column storeId is of the type string. Identify the error.Code block:storesDF.withColumn(‚ÄústoreId‚Äù, cast(col(‚ÄústoreId‚Äù), StringType()))",
        "choices": [
            "A.Calls to withColumn() cannot create a new column of the same name on which it is operating.",
            "B.DataFrame columns cannot be converted to a new type inside of a call to withColumn().",
            "C.The call to StringType should not be followed by parentheses.",
            "D.The column name storeId inside the col() operation should not be quoted.",
            "E.The cast() operation is a method in the Column class rather than a standalone function."
        ],
        "correct_answer": "C",
        "user_data": [
            {
                "voted_answers": "E",
                "vote_count": 3,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 21 discussion.html",
        "question": "Which of the following code blocks returns a DataFrame containing only the rows from DataFrame storesDF where the value in column sqft is less than or equal to 25,000 OR the value in column customerSatisfaction is greater than or equal to 30?",
        "choices": [
            "A.storesDF.filter(col(\"sqft\") <= 25000 | col(\"customerSatisfaction\") >= 30)",
            "B.storesDF.filter(col(\"sqft\") <= 25000 or col(\"customerSatisfaction\") >= 30)",
            "C.storesDF.filter(sqft <= 25000 or customerSatisfaction >= 30)",
            "D.storesDF.filter(col(sqft) <= 25000 | col(customerSatisfaction) >= 30)",
            "E.storesDF.filter((col(\"sqft\") <= 25000) | (col(\"customerSatisfaction\") >= 30))"
        ],
        "correct_answer": "E",
        "user_data": [
            {
                "voted_answers": "E",
                "vote_count": 7,
                "is_most_voted": true
            },
            {
                "voted_answers": "A",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 92 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 54 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 57 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 43 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 94 discussion.html",
        "question": "Which of the following operations performs a cross join on two DataFrames?",
        "choices": [
            "A.DataFrame.join()",
            "B.The standalone join() function",
            "C.The standalone crossJoin() function",
            "D.DataFrame.crossJoin()",
            "E.DataFrame.merge()"
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "D",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 59 discussion.html",
        "question": "A data engineer wants to create a relational object by pulling data from two tables. The relational object does not need to be used by other data engineers in other sessions. In order to save on storage costs, the data engineer wants to avoid copying and storing physical data.Which of the following relational objects should the data engineer create?",
        "choices": [
            "A.Spark SQL Table",
            "B.View",
            "C.Database",
            "D.Temporary view",
            "E.Delta Table"
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "D",
                "vote_count": 4,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 37 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 165 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 99 discussion.html",
        "question": "Which of the following code blocks returns a new Data Frame from DataFrame storesDF with no duplicate rows?",
        "choices": [
            "A.storesDF.removeDuplicates()",
            "B.storesDF.getDistinct()",
            "C.storesDF.duplicates.drop()",
            "D.storesDF.duplicates()",
            "E.storesDF.dropDuplicates()"
        ],
        "correct_answer": "E",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 46 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 62 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 70 discussion.html",
        "question": "A data ingestion task requires a one-TB JSON dataset to be written out to Parquet with a target part-file size of 512 MB. Because Parquet is being used instead of Delta Lake, built-in file-sizing features such as Auto-Optimize & Auto-Compaction cannot be used.Which strategy will yield the best performance without shuffling data?",
        "choices": [
            "A.Set spark.sql.files.maxPartitionBytes to 512 MB, ingest the data, execute the narrow transformations, and then write to parquet.",
            "B.Set spark.sql.shuffle.partitions to 2,048 partitions (1TB*1024*1024/512), ingest the data, execute the narrow transformations, optimize the data by sorting it (which automatically repartitions the data), and then write to parquet.",
            "C.Set spark.sql.adaptive.advisoryPartitionSizeInBytes to 512 MB bytes, ingest the data, execute the narrow transformations, coalesce to 2,048 partitions (1TB*1024*1024/512), and then write to parquet.",
            "D.Ingest the data, execute the narrow transformations, repartition to 2,048 partitions (1TB* 1024*1024/512), and then write to parquet.",
            "E.Set spark.sql.shuffle.partitions to 512, ingest the data, execute the narrow transformations, and then write to parquet."
        ],
        "correct_answer": "B",
        "user_data": [
            {
                "voted_answers": "A",
                "vote_count": 16,
                "is_most_voted": true
            },
            {
                "voted_answers": "D",
                "vote_count": 8,
                "is_most_voted": false
            },
            {
                "voted_answers": "C",
                "vote_count": 4,
                "is_most_voted": false
            },
            {
                "voted_answers": "B",
                "vote_count": 2,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 109 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 6 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 177 discussion.html",
        "question": "You want to rebuild your batch pipeline for structured data on Google Cloud. You are using PySpark to conduct data transformations at scale, but your pipelines are taking over twelve hours to run. To expedite development and pipeline run time, you want to use a serverless tool and SOL syntax. You have already moved your raw data into Cloud Storage. How should you build the pipeline on Google Cloud while meeting speed and processing requirements?",
        "choices": [
            "A.Convert your PySpark commands into SparkSQL queries to transform the data, and then run your pipeline on Dataproc to write the data into BigQuery.",
            "B.Ingest your data into Cloud SQL, convert your PySpark commands into SparkSQL queries to transform the data, and then use federated quenes from BigQuery for machine learning.",
            "C.Ingest your data into BigQuery from Cloud Storage, convert your PySpark commands into BigQuery SQL queries to transform the data, and then write the transformations to a new table.",
            "D.Use Apache Beam Python SDK to build the transformation pipelines, and write the data into BigQuery."
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "C",
                "vote_count": 30,
                "is_most_voted": true
            },
            {
                "voted_answers": "A",
                "vote_count": 5,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 176 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 22 discussion.html",
        "question": "Which of the following code blocks returns a new DataFrame from DataFrame storesDF where column storeId is of the type string?",
        "choices": [
            "A.storesDF.withColumn(\"storeId, cast(col(\"storeId\"), StringType()))",
            "B.storesDF.withColumn(\"storeId, col(\"storeId\").cast(StringType()))",
            "C.storesDF.withColumn(\"storeId, cast(storeId).as(StringType)",
            "D.storesDF.withColumn(\"storeId, col(storeId).cast(StringType)",
            "E.storesDF.withColumn(\"storeId, cast(\"storeId\").as(StringType()))"
        ],
        "correct_answer": "B",
        "user_data": [
            {
                "voted_answers": "B",
                "vote_count": 6,
                "is_most_voted": true
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 122 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 20 discussion.html",
        "question": "Which of the following code blocks returns a DataFrame containing only the rows from DataFrame storesDF where the value in column sqft is less than or equal to 25,000?",
        "choices": [
            "A.storesDF.filter(\"sqft\" <= 25000)",
            "B.storesDF.filter(sqft > 25000)",
            "C.storesDF.where(storesDF[sqft] > 25000)",
            "D.storesDF.where(sqft > 25000)",
            "E.storesDF.filter(col(\"sqft\") <= 25000)"
        ],
        "correct_answer": "E",
        "user_data": [
            {
                "voted_answers": "E",
                "vote_count": 6,
                "is_most_voted": true
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 151 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 17 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 133 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 15 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 144 discussion.html",
        "question": "Which of the following code blocks returns a new DataFrame that is the result of a cross join between DataFrame storesDF and DataFrame employeesDF?",
        "choices": [
            "A.storesDF.crossJoin(employeesDF)",
            "B.storesDF.join(employeesDF, \"storeId\", \"cross\")",
            "C.crossJoin(storesDF, employeesDF)",
            "D.join(storesDF, employeesDF, \"cross\")",
            "E.storesDF.join(employeesDF, \"cross\")"
        ],
        "correct_answer": "A",
        "user_data": [
            {
                "voted_answers": "A",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 117 discussion.html",
        "question": "The code block shown below should return a DataFrame containing all columns from DataFrame storesDF except for column sqft and column customerSatisfaction. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.Code block:__1__.__2__(__3__)",
        "choices": [
            "A.1. drop2. storesDF3. col(‚Äúsqft‚Äù), col(‚ÄúcustomerSatisfaction‚Äù)",
            "B.1. storesDF2. drop3. sqft, customerSatisfaction",
            "C.1. storesDF2. drop3. ‚Äúsqft‚Äù, ‚ÄúcustomerSatisfaction‚Äù",
            "D.1. storesDF2. drop3. col(sqft), col(customerSatisfaction)",
            "E.1. drop2. storesDF3. col(sqft), col(customerSatisfaction)"
        ],
        "correct_answer": "C",
        "user_data": [
            {
                "voted_answers": "C",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 160 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 69 discussion.html",
        "question": "Which of the following code blocks returns a new DataFrame from DataFrame storesDF where column modality is the constant string \"PHYSICAL\"? Assume DataFrame storesDF is the only defined language variable.",
        "choices": [
            "A.storesDF.withColumn(\"modality\", lit(PHYSICAL))",
            "B.storesDF.withColumn(\"modality\", col(\"PHYSICAL\"))",
            "C.storesDF.withColumn(\"modality\", lit(\"PHYSICAL\"))",
            "D.storesDF.withColumn(\"modality\", StringType(\"PHYSICAL\"))",
            "E.storesDF.withColumn(\"modality\", \"PHYSICAL\")"
        ],
        "correct_answer": "C",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 157 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 45 discussion.html",
        "question": "The code block shown below contains an error. The code block is intended to cache DataFrame storesDF only in Spark‚Äôs memory and then return the number of rows in the cached DataFrame. Identify the error.Code block:storesDF.cache().count()",
        "choices": [
            "A.The cache() operation caches DataFrames at the MEMORY_AND_DISK level by default ‚Äì the storage level must be specified to MEMORY_ONLY as an argument to cache().",
            "B.The cache() operation caches DataFrames at the MEMORY_AND_DISK level by default ‚Äì the storage level must be set via storesDF.storageLevel prior to calling cache().",
            "C.The storesDF DataFrame has not been checkpointed ‚Äì it must have a checkpoint in order to be cached.",
            "D.DataFrames themselves cannot be cached ‚Äì DataFrame storesDF must be cached as a table.",
            "E.The cache() operation can only cache DataFrames at the MEMORY_AND_DISK level (the default) ‚Äì persist() should be used instead."
        ],
        "correct_answer": "B",
        "user_data": [
            {
                "voted_answers": "E",
                "vote_count": 2,
                "is_most_voted": false
            },
            {
                "voted_answers": "B",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 180 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 169 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 68 discussion.html",
        "question": "The code block shown below should return a new DataFrame from DataFrame storesDF where column storeId is of the type string. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.Code block:storesDF.__1__(\"storeId\", __2__(\"storeId\").__3__(__4__)",
        "choices": [
            "A.1. withColumn2. col3. cast4. StringType()",
            "B.1. withColumn2. cast3. col4. StringType()",
            "C.1. newColumn2. col3. cast4. StringType()",
            "D.1. withColumn2. cast3. col4. StringType",
            "E.1. withColumn2. col3. cast4. StringType"
        ],
        "correct_answer": "A",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 137 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 25 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 182 discussion.html",
        "question": "Which of the following describes why garbage collection in Spark is important?",
        "choices": [
            "A.Logical results will be incorrect if inaccurate data is not collected and removed from the Spark job.",
            "B.Spark jobs will fail or run slowly if inaccurate data is not collected and removed from the Spark job.",
            "C.Spark jobs will fail or run slowly if memory is not available for new objects to be created.",
            "D.Spark jobs will produce inaccurate results if there are too many different transformations called before a single action.",
            "E.Spark jobs will produce inaccurate results if memory is not available for new tasks to run and complete."
        ],
        "correct_answer": "C",
        "user_data": [
            {
                "voted_answers": "C",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 142 discussion.html",
        "question": "The code block shown below should adjust the number of partitions used in wide transformations like join() to 32. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.Code block:__1__(__2__, __3__)",
        "choices": [
            "A.1. spark.conf.get2. \"spark.sql.shuffle.partitions\"3. \"32\"",
            "B.1. spark.conf.set2. \"spark.default.parallelism\"3. 32",
            "C.1. spark.conf.text2. \"spark.default.parallelism\"3. \"32\"",
            "D.1. spark.conf.set2. \"spark.default.parallelism\"3. \"32\"",
            "E.1. spark.conf.set2. \"spark.sql.shuffle.partitions\"3. \"32\""
        ],
        "correct_answer": "E",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 91 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 35 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 128 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 24 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 174 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 138 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 156 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 175 discussion.html",
        "question": "Which of the following code blocks returns a DataFrame containing a column month, an integer representation of the month from column openDate from DataFrame storesDF?Note that column openDate is of type integer and represents a date in the UNIX epoch format ‚Äî the number of seconds since midnight on January 1 st, 1970.A sample of storesDF is displayed below:",
        "choices": [
            "A.storesDF.withColumn(\"month\", getMonth(col(\"openDate\")))",
            "B.storesDF.withColumn(\"month\", substr(col(\"openDate\"), 4, 2))",
            "C.(storesDF.withColumn(\"openDateFormat\", col(\"openDate\").cast(\"Date\")).withColumn(\"month\", month(col(\"openDateFormat\"))))",
            "D.(storesDF.withColumn(\"openTimestamp\", col(\"openDate\").cast(\"Timestamp\")).withColumn(\"month\", month(col(\"openTimestamp\"))))",
            "E.storesDF.withColumn(\"month\", month(col(\"openDate\")))"
        ],
        "correct_answer": "D",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 14 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 12 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 83 discussion.html",
        "question": "Which of the following code blocks creates and registers a SQL UDF named \"ASSESS_PERFORMANCE\" using the Scala function assessPerformance() and applies it to column customerSatisfaction in table stores?",
        "choices": [
            "A.spark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)spark.sql(\"SELECT customerSatisfaction, ASSESS_PERFORMANCE(customerSatisfaction) AS result FROM stores\")",
            "B.spark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)",
            "C.spark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)spark.sql(\"SELECT customerSatisfaction, assessPerformance(customerSatisfaction) AS result FROM stores\")",
            "D.spark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)storesDF.withColumn(\"result\", assessPerformance(col(\"customerSatisfaction\")))",
            "E.spark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)storesDF.withColumn(\"result\", ASSESS_PERFORMANCE(col(\"customerSatisfaction\")))"
        ],
        "correct_answer": "A",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 31 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 150 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 55 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 11 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 102 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 105 discussion.html",
        "question": "The data science team has created and logged a production model using MLflow. The model accepts a list of column names and returns a new column of type DOUBLE.The following code correctly imports the production model, loads the customers table containing the customer_id key column into a DataFrame, and defines the feature columns needed for the model.Which code block will output a DataFrame with the schema \"customer_id LONG, predictions DOUBLE\"?",
        "choices": [
            "A.df.map(lambda x:model(x[columns])).select(\"customer_id, predictions\")",
            "B.df.select(\"customer_id\", model(*columns).alias(\"predictions\"))",
            "C.model.predict(df, columns)",
            "D.df.select(\"customer_id\", pandas_udf(model, columns).alias(\"predictions\"))",
            "E.df.apply(model, columns).select(\"customer_id, predictions\")"
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "B",
                "vote_count": 8,
                "is_most_voted": true
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 10 discussion.html",
        "question": "Which of the following DataFrame operations is classified as a wide transformation?",
        "choices": [
            "A.DataFrame.filter()",
            "B.DataFrame.join()",
            "C.DataFrame.select()",
            "D.DataFrame.drop()",
            "E.DataFrame.union()"
        ],
        "correct_answer": "B",
        "user_data": [
            {
                "voted_answers": "B",
                "vote_count": 11,
                "is_most_voted": true
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 78 discussion.html",
        "question": "Which of the following code blocks returns a DataFrame sorted alphabetically based on column division?",
        "choices": [
            "A.storesDF.sort(\"division\")",
            "B.storesDF.orderBy(desc(\"division\"))",
            "C.storesDF.orderBy(col(\"division\").desc())",
            "D.storesDF.orderBy(\"division\", ascending - true)",
            "E.storesDF.sort(desc(\"division\"))"
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "A",
                "vote_count": 2,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 13 discussion.html",
        "question": "Which of the following cluster configurations is most likely to experience an out-of-memory error in response to data skew in a single partition?Note: each configuration has roughly the same compute power using 100 GB of RAM and 200 cores.",
        "choices": [
            "A.Scenario #4",
            "B.Scenario #5",
            "C.Scenario #6",
            "D.More information is needed to determine an answer.",
            "E.Scenario #1"
        ],
        "correct_answer": "C",
        "user_data": [
            {
                "voted_answers": "C",
                "vote_count": 12,
                "is_most_voted": true
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 135 discussion.html",
        "question": "Which of the following code blocks returns a new DataFrame where column division is the first two characters of column division in DataFrame storesDF?",
        "choices": [
            "A.storesDF.withColumn(‚Äúdivision‚Äù, substr(col(‚Äúdivision‚Äù), 0, 2))",
            "B.storesDF.withColumn(‚Äúdivision‚Äù, susbtr(col(‚Äúdivision‚Äù), 1, 2))",
            "C.storesDF,withColumn(‚Äúdivision‚Äù, col(‚Äúdivision‚Äù).substr(0, 3))",
            "D.storesDF.withColumn(‚Äúdivision‚Äù, col(‚Äúdivision‚Äù).substr(0, 2))",
            "E.storesDF.withColumn(‚Äúdivision‚Äù, col(‚Äúdivision‚Äù).substr(l, 2))"
        ],
        "correct_answer": "E",
        "user_data": [
            {
                "voted_answers": "D",
                "vote_count": 4,
                "is_most_voted": false
            },
            {
                "voted_answers": "E",
                "vote_count": 2,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 163 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 1 discussion.html",
        "question": "Which of the following describes the Spark driver?",
        "choices": [
            "A.The Spark driver is responsible for performing all execution in all execution modes ‚Äì it is the entire Spark application.",
            "B.The Spare driver is fault tolerant ‚Äì if it fails, it will recover the entire Spark application.",
            "C.The Spark driver is the coarsest level of the Spark execution hierarchy ‚Äì it is synonymous with the Spark application.",
            "D.The Spark driver is the program space in which the Spark application‚Äôs main method runs coordinating the Spark entire application.",
            "E.The Spark driver is horizontally scaled to increase overall processing throughput of a Spark application."
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "D",
                "vote_count": 3,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 41 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 44 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 139 discussion.html",
        "question": "Which of the following code blocks returns a 15 percent sample of rows from DataFrame storesDF without replacement?",
        "choices": [
            "A.storesDF.sample(True, fraction = 0.15)",
            "B.storesDF.sample(fraction = 0.15)",
            "C.storesDF.sampleBy(fraction = 0.15)",
            "D.storesDF.sample(fraction = 0.10)",
            "E.storesDF.sample()"
        ],
        "correct_answer": "B",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 152 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 23 discussion.html",
        "question": "Which of the following code blocks returns a new DataFrame with a new column employeesPerSqft that is the quotient of column numberOfEmployees and column sqft, both of which are from DataFrame storesDF? Note that column employeesPerSqft is not in the original DataFrame storesDF.",
        "choices": [
            "A.storesDF.withColumn(\"employeesPerSqft\", col(\"numberOfEmployees\") / col(\"sqft\"))",
            "B.storesDF.withColumn(\"employeesPerSqft\", \"numberOfEmployees\" / \"sqft\")",
            "C.storesDF.select(\"employeesPerSqft\", \"numberOfEmployees\" / \"sqft\")",
            "D.storesDF.select(\"employeesPerSqft\", col(\"numberOfEmployees\") / col(\"sqft\"))",
            "E.storesDF.withColumn(col(\"employeesPerSqft\"), col(\"numberOfEmployees\") / col(\"sqft\"))"
        ],
        "correct_answer": "A",
        "user_data": [
            {
                "voted_answers": "A",
                "vote_count": 4,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 103 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 168 discussion.html",
        "question": "The code block shown below should return a DataFrame containing only the rows from DataFrame storesDF where the value in column sqft is less than or equal to 25,000 AND the value in column customerSatisfaction is greater than or equal to 30. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.Code block:storesDF.__1__(__2__ __3__ __4__)",
        "choices": [
            "A.1. filter2. (col(\"sqft\") <= 25000)3. &4. (col(\"customerSatisfaction\") >= 30)",
            "B.1. filter2. (col(\"sqft\") <= 250003. &4. col(\"customerSatisfaction\") >= 30",
            "C.1. filter2. (col(\"sqft\") <= 25000)3. and4. (col(\"customerSatisfaction\") >= 30)",
            "D.1. drop2. (col(sqft) <= 25000)3. &4. (col(customerSatisfaction) >= 30)",
            "E.1. filter2. col(\"sqft\") <= 250003. and4. col(\"customerSatisfaction\") >= 30"
        ],
        "correct_answer": "A",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 73 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 143 discussion.html",
        "question": "Which of the following code blocks returns a DataFrame containing a column openDateString, a string representation of Java‚Äôs SimpleDateFormat?Note that column openDate is of type integer and represents a date in the UNIX epoch format ‚Äî the number of seconds since midnight on January 1st, 1970.An example of Java's SimpleDateFormat is \"Sunday, Dec 4, 2008 1:05 pm\".A sample of storesDF is displayed below:",
        "choices": [
            "A.storesDF.withColumn(\"openDatestring\", from unixtime(col(\"openDate‚Äú), ‚ÄúEEEE, MMM d, yyyy h:mm a\"))",
            "B.storesDF.withColumn(\"openDateString\", from_unixtime(col(\"openDate‚Äú), \"EEEE, MMM d, yyyy h:mm a\", TimestampType()))",
            "C.storesDF.withColumn(\"openDateString\", date(col(\"openDate\"), \"EEEE, MMM d, yyyy h:mm a\"))",
            "D.storesDF.newColumn(col(\"openDateString\"), from_unixtime(\"openDate\", \"EEEE, MMM d, yyyy h:mm a\"))",
            "E.storesDF.withColumn(\"openDateString\", date(col(\"openDate‚Äú), \"EEEE, MMM d, yyyy h:mm a\", TimestampType))"
        ],
        "correct_answer": "A",
        "user_data": [
            {
                "voted_answers": "B",
                "vote_count": 2,
                "is_most_voted": false
            },
            {
                "voted_answers": "A",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 140 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 84 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 71 discussion.html",
        "question": "The code block shown below should return a new DataFrame where single quotes in column storeSlogan have been replaced with double quotes. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.A sample of DataFrame storesDF is below:Code block:storesDF.__1__(__2__, __3__(__4__, __5__, __6__))",
        "choices": [
            "A.1. withColumn2. \"storeSlogan\"3. regexp_extract4. col(\"storeSlogan\")5. \"\\\"\"6. \"'\"",
            "B.1. newColumn2. storeSlogan3. regexp_extract4. col(storeSlogan)5. \"\\\"\"6. \"'\"",
            "C.1. withColumn2. \"storeSlogan\"3. regexp_replace4. col(\"storeSlogan\")5. \"\\\"\"6. \"'\"",
            "D.1. withColumn2. \"storeSlogan\"3. regexp_replace4. col(\"storeSlogan\")5. \"'\"6. \"\\\"\"",
            "E.1. withColumn2. \"storeSlogan\"3. regexp_extract4. col(\"storeSlogan\")5. \"'\"6. \"\\\"\""
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "D",
                "vote_count": 3,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 173 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 125 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 172 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 154 discussion.html",
        "question": "The code block shown below should return a new DataFrame where rows in DataFrame storesDF containing at least one missing value have been dropped. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.Code block:StoresDF.__1__.__2__(__3__ = __4__)",
        "choices": [
            "A.1. na2. drop3. subset4. \"any\"",
            "B.1. na2. drop3. how4. \"all\"",
            "C.1. na2. drop3. subset4. \"all\"",
            "D.1. na2. drop3. how4. \"any\"",
            "E.1. drop2. na3. how4. \"any\""
        ],
        "correct_answer": "D",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 40 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 179 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 113 discussion.html",
        "question": "Which of the following code blocks writes DataFrame storesDF to file path filePath as parquet overwriting any existing files in that location?",
        "choices": [
            "A.storesDF.write(filePath, mode = ‚Äúoverwrite‚Äù)",
            "B.storesDF.write().mode(‚Äúoverwrite‚Äù).parquet(filePath)",
            "C.storesDF.write.mode(‚Äúoverwrite‚Äù).parquet(filePath)",
            "D.storesDF.write.option(‚Äúparquet‚Äù, ‚Äúoverwrite‚Äù).path(filePath)",
            "E.storesDF.write.mode(‚Äúoverwrite‚Äù).path(filePath)"
        ],
        "correct_answer": "C",
        "user_data": [
            {
                "voted_answers": "C",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 74 discussion.html",
        "question": "Which of the following must be specified when creating a new Delta Live Tables pipeline?",
        "choices": [
            "A.A key-value pair configuration",
            "B.The preferred DBU/hour cost",
            "C.A path to cloud storage location for the written data",
            "D.A location of a target database for the written data",
            "E.At least one notebook library to be executed"
        ],
        "correct_answer": "E",
        "user_data": [
            {
                "voted_answers": "E",
                "vote_count": 14,
                "is_most_voted": true
            },
            {
                "voted_answers": "C",
                "vote_count": 7,
                "is_most_voted": true
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 49 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 19 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 146 discussion.html",
        "question": "In what order should the below lines of code be run in order to read a parquet at the file path filePath into a DataFrame?Lines of code:1. storesDF2. .load(filePath, source = \"parquet\")3. .read \\4. spark \\5. .read() \\6. .parquet(filePath)",
        "choices": [
            "A.1, 5, 2",
            "B.4, 5, 2",
            "C.4, 3, 6",
            "D.4, 5, 6",
            "E.4, 3, 2"
        ],
        "correct_answer": "C",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 121 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 30 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 162 discussion.html",
        "question": "Which of the following operations can be used to perform a left join on two DataFrames?",
        "choices": [
            "A.DataFrame.join()",
            "B.DataFrame.crossJoin()",
            "C.DataFrame.merge()",
            "D.DataFrame.leftJoin()",
            "E.Standalone join() function"
        ],
        "correct_answer": "A",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 148 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 104 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 161 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 3 discussion.html",
        "question": "Which of the following will occur if there are more slots than there are tasks?",
        "choices": [
            "A.The Spark job will likely not run as efficiently as possible.",
            "B.The Spark application will fail ‚Äì there must be at least as many tasks as there are slots.",
            "C.Some executors will shut down and allocate all slots on larger executors first.",
            "D.More tasks will be automatically generated to ensure all slots are being used.",
            "E.The Spark job will use just one single slot to perform all tasks."
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "A",
                "vote_count": 12,
                "is_most_voted": true
            },
            {
                "voted_answers": "E",
                "vote_count": 2,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 32 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 16 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 171 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 93 discussion.html",
        "question": "You are performing a join operation to combine values from a static userLookup table with a streaming DataFrame streamingDF.Which code block attempts to perform an invalid stream-static join?",
        "choices": [
            "A.userLookup.join(streamingDF, [\"userid\"], how=\"inner\")",
            "B.streamingDF.join(userLookup, [\"user_id\"], how=\"outer\")",
            "C.streamingDF.join(userLookup, [\"user_id‚Äù], how=\"left\")",
            "D.streamingDF.join(userLookup, [\"userid\"], how=\"inner\")",
            "E.userLookup.join(streamingDF, [\"user_id\"], how=\"right\")"
        ],
        "correct_answer": "E",
        "user_data": [
            {
                "voted_answers": "B",
                "vote_count": 11,
                "is_most_voted": true
            },
            {
                "voted_answers": "E",
                "vote_count": 2,
                "is_most_voted": false
            },
            {
                "voted_answers": "D",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 4 discussion.html",
        "question": "Which of the following is the most granular level of the Spark execution hierarchy?",
        "choices": [
            "A.Task",
            "B.Executor",
            "C.Node",
            "D.Job",
            "E.Slot"
        ],
        "correct_answer": "A",
        "user_data": [
            {
                "voted_answers": "A",
                "vote_count": 4,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 66 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 97 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 89 discussion.html",
        "question": "A data engineer needs access to a table new_table, but they do not have the correct permissions. They can ask the table owner for permission, but they do not know who the table owner is.Which of the following approaches can be used to identify the owner of new_table?",
        "choices": [
            "A.Review the Permissions tab in the table's page in Data Explorer",
            "B.All of these options can be used to identify the owner of the table",
            "C.Review the Owner field in the table's page in Data Explorer",
            "D.Review the Owner field in the table's page in the cloud storage solution",
            "E.There is no way to identify the owner of the table"
        ],
        "correct_answer": "C",
        "user_data": [
            {
                "voted_answers": "C",
                "vote_count": 3,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 47 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 86 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 56 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 123 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 60 discussion.html",
        "question": "In what order should the below lines of code be run in order to read a JSON file at the file path filePath into a DataFrame with the specified schema schema?Lines of code:1. .json(filePath, schema = schema)2. .storesDF3. .spark \\4. .read() \\5. .read \\6. .json(filePath, format = schema)",
        "choices": [
            "A.3, 5, 6",
            "B.2, 4, 1",
            "C.3, 5, 1",
            "D.2, 5, 1",
            "E.3, 4, 1"
        ],
        "correct_answer": "C -",
        "user_data": [
            {
                "voted_answers": "C",
                "vote_count": 3,
                "is_most_voted": false
            },
            {
                "voted_answers": "B",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 111 discussion.html",
        "question": "Which describes a method of installing a Python package scoped at the notebook level to all nodes in the currently active cluster?",
        "choices": [
            "A.Run source env/bin/activate in a notebook setup script",
            "B.Use b in a notebook cell",
            "C.Use %pip install in a notebook cell",
            "D.Use %sh pip install in a notebook cell",
            "E.Install libraries from PyPI using the cluster UI"
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "C",
                "vote_count": 5,
                "is_most_voted": true
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 112 discussion.html",
        "question": "The code block shown below contains an error. The code block is intended to return a new DataFrame that is the result of a position-wise union between DataFrame storesDF and DataFrame acquiredStoresDF.",
        "choices": [
            "A.concat(storesDF, acquiredStoresDF)",
            "B.storesDF.unionByName(acquiredStoresDF)",
            "C.union(storesDF, acquiredStoresDF)",
            "D.unionAll(storesDF, acquiredStoresDF)",
            "E.storesDF.union(acquiredStoresDF)"
        ],
        "correct_answer": "E",
        "user_data": [
            {
                "voted_answers": "E",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 130 discussion.html",
        "question": "Spark's execution/deployment mode determines where the driver and executors are physically located when a Spark application is run. Which of the following Spark execution/deployment modes does not exist? If they all exist, please indicate so with Response E.",
        "choices": [
            "A.Client mode",
            "B.Cluster mode",
            "C.Standard mode",
            "D.Local mode",
            "E.All of these execution/deployment modes exist"
        ],
        "correct_answer": "C",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 166 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 26 discussion.html",
        "question": "Which of the following code blocks returns a new DataFrame where column productCategories only has one word per row, resulting in a DataFrame with many more rows than DataFrame storesDF?A sample of storesDF is displayed below:",
        "choices": [
            "A.storesDF.withColumn(\"productCategories\", explode(col(\"productCategories\")))",
            "B.storesDF.withColumn(\"productCategories\", split(col(\"productCategories\")))",
            "C.storesDF.withColumn(\"productCategories\", col(\"productCategories\").explode())",
            "D.storesDF.withColumn(\"productCategories\", col(\"productCategories\").split())",
            "E.storesDF.withColumn(\"productCategories\", explode(\"productCategories\"))"
        ],
        "correct_answer": "A",
        "user_data": [
            {
                "voted_answers": "A",
                "vote_count": 7,
                "is_most_voted": true
            },
            {
                "voted_answers": "E",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 88 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 52 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 90 discussion.html",
        "question": "The code block shown below should return a new DataFrame that is the result of an inner join between DataFrame storeDF and DataFrame employeesDF on column storeId. Choose the response chat correctly fills in the numbered blanks within the code block to complete this task.Code block:storesDF.__1__(__2__, __3__, __4__)",
        "choices": [
            "A.1. join2. employeesDF3. \"inner\"4. storesDF.storeId === employeesDF.storeId",
            "B.1. join2. employeesDF3. \"storeId\"4. \"inner\"",
            "C.1. merge2. employeesDF3. \"storeId\"4. \"inner\"",
            "D.1. join2. employeesDF3. \"inner\"4. \"storeId\"",
            "E.1. join2. employeesDF3. \"inner\"4. \"storeId\""
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "B",
                "vote_count": 6,
                "is_most_voted": true
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 100 discussion.html",
        "question": "The code block shown below contains an error. The code block is intended to return the exact number of distinct values in column division in DataFrame storesDF. Identify the error.Code block:storesDF.agg(approx_count_distinct(col(‚Äúdivision‚Äù)).alias(‚ÄúdivisionDistinct‚Äù))",
        "choices": [
            "A.The approx_count_distinct() operation needs a second argument to set the rsd parameter to ensure it returns the exact number of distinct values.",
            "B.There is no alias() operation for the approx_count_distinct() operation's output.",
            "C.There is no way to return an exact distinct number in Spark because the data Is distributed across partitions.",
            "D.The approx_count_distinct()operation is not a standalone function - it should be used as a method from a Column object.",
            "E.The approx_count_distinct() operation cannot determine an exact number of distinct values in a column."
        ],
        "correct_answer": "A",
        "user_data": [
            {
                "voted_answers": "E",
                "vote_count": 4,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 115 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 75 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 178 discussion.html",
        "question": "Which of the following describes why garbage collection in Spark is important?",
        "choices": [
            "A.Logical results will be incorrect if inaccurate data is not collected and removed from the Spark job.",
            "B.Spark jobs will fail or run slowly if inaccurate data is not collected and removed from the Spark job.",
            "C.Spark jobs will fail or run slowly if memory is not available for new objects to be created.",
            "D.Spark jobs will produce inaccurate results if there are too many different transformations called before a single action.",
            "E.Spark jobs will produce inaccurate results if memory is not available for new tasks to run and complete."
        ],
        "correct_answer": "C",
        "user_data": [
            {
                "voted_answers": "C",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 38 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 159 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 51 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 80 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 155 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 110 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 127 discussion.html",
        "question": "Which of the following cluster configurations will induce the least network traffic during a shuffle operation?Note: each configuration has roughly the same compute power using 100GB of RAM and 200 cores.",
        "choices": [
            "A.This cannot be determined without knowing the number of partitions.",
            "B.Scenario 5",
            "C.Scenario 1",
            "D.Scenario 4",
            "E.Scenario 6"
        ],
        "correct_answer": "C",
        "user_data": [
            {
                "voted_answers": "C",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 126 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 53 discussion.html",
        "question": "Which of the following is stored in the Databricks customer's cloud account?",
        "choices": [
            "A.Databricks web application",
            "B.Cluster management metadata",
            "C.Repos",
            "D.Data",
            "E.Notebooks"
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "D",
                "vote_count": 6,
                "is_most_voted": true
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 63 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 18 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 76 discussion.html",
        "question": "The code block shown below should return a new DataFrame with the mean of column sqft from DataFrame storesDF in column sqftMean. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.Code block:storesDF.__1__(__2__(__3__).alias(\"sqftMean\"))",
        "choices": [
            "A.1. agg2. mean3. col(\"sqft\")",
            "B.1. withColumn2. mean3. col(\"sqft\")",
            "C.1. agg2. average3. col(\"sqft\")",
            "D.1. mean2. col3. \"sqft\"",
            "E.1. agg2. mean3. \"sqft\""
        ],
        "correct_answer": "A",
        "user_data": [
            {
                "voted_answers": "A",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 136 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 116 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 120 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 58 discussion.html",
        "question": "Which of the following describes a benefit of creating an external table from Parquet rather than CSV when using a CREATE TABLE AS SELECT statement?",
        "choices": [
            "A.Parquet files can be partitioned",
            "B.CREATE TABLE AS SELECT statements cannot be used on files",
            "C.Parquet files have a well-defined schema",
            "D.Parquet files have the ability to be optimized",
            "E.Parquet files will become Delta tables"
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "C",
                "vote_count": 14,
                "is_most_voted": true
            },
            {
                "voted_answers": "B",
                "vote_count": 1,
                "is_most_voted": false
            },
            {
                "voted_answers": "D",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 2 discussion.html",
        "question": "Which of the following describes the relationship between nodes and executors?",
        "choices": [
            "A.Executors and nodes are not related.",
            "B.Anode is a processing engine running on an executor.",
            "C.An executor is a processing engine running on a node.",
            "D.There are always the same number of executors and nodes.",
            "E.There are always more nodes than executors."
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "C",
                "vote_count": 14,
                "is_most_voted": true
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 118 discussion.html",
        "question": "Which of the following describes the difference between DataFrame.repartition(n) and DataFrame.coalesce(n)?",
        "choices": [
            "A.DataFrame.repartition(n) will split a DataFrame into n number of new partitions with data distributed evenly.DataFrame.coalesce(n) will more quickly combine the existing partitions of a DataFrame but might result in an uneven distribution of data across the new partitions.",
            "B.While the results are similar, DataFrame.repartition(n) will be more efficient than DataFrame.coalesce(n) because it can partition a Data Frame by the column.",
            "C.DataFrame.repartition(n) will split a Data Frame into any number of new partitions while minimizing shuffling.DataFrame.coalesce(n) will split a DataFrame onto any number of new partitions utilizing a full shuffle.",
            "D.While the results are similar, DataFrame.repartition(n) will be less efficient than DataFrame.coalesce(n) because it can partition a Data Frame by the column.",
            "E.DataFrame.repartition(n) will combine the existing partitions of a DataFrame but may result in an uneven distribution of data across the new partitions.DataFrame.coalesce(n) will more slowly split a Data Frame into n number of new partitions with data distributed evenly."
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "A",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 77 discussion.html",
        "question": "A data engineer and data analyst are working together on a data pipeline. The data engineer is working on the raw, bronze, and silver layers of the pipeline using Python, and the data analyst is working on the gold layer of the pipeline using SQL. The raw source of the pipeline is a streaming input. They now want to migrate their pipeline to use Delta Live Tables.Which of the following changes will need to be made to the pipeline when migrating to Delta Live Tables?",
        "choices": [
            "A.None of these changes will need to be made",
            "B.The pipeline will need to stop using the medallion-based multi-hop architecture",
            "C.The pipeline will need to be written entirely in SQL",
            "D.The pipeline will need to use a batch source in place of a streaming source",
            "E.The pipeline will need to be written entirely in Python"
        ],
        "correct_answer": "B",
        "user_data": [
            {
                "voted_answers": "A",
                "vote_count": 9,
                "is_most_voted": true
            },
            {
                "voted_answers": "D",
                "vote_count": 7,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 39 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 167 discussion.html",
        "question": "Which of the following describes executors?",
        "choices": [
            "A.Executors are the communication pathways from the driver node to the worker nodes.",
            "B.Executors are the most granular level of execution in the Spark execution hierarchy.",
            "C.Executors always have a one-to-one relationship with worker nodes.",
            "D.Executors are synonymous with worker nodes.",
            "E.Executors are processing engine instances for performing data computations which run on a worker node."
        ],
        "correct_answer": "E",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 33 discussion.html",
        "question": "Which of the following operations can be used to return the number of rows in a DataFrame?",
        "choices": [
            "A.DataFrame.numberOfRows()",
            "B.DataFrame.n()",
            "C.DataFrame.sum()",
            "D.DataFrame.count()",
            "E.DataFrame.countDistinct()"
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "D",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 108 discussion.html",
        "question": "The code block shown below should cache DataFrame storesDF only in Spark's memory. Choose the response that correctly fil ls in the numbered blanks within the code block to complete this task.Code block:__1__.__2__(__3__).count()",
        "choices": [
            "A.1. storesDF2. cache3. StorageLevel.MEMORY_ONLY",
            "B.1. storesDF2. storageLevel3. cache",
            "C.1. storesDF2. cache3. Nothing",
            "D.1. storesDF2. persist3. Nothing",
            "E.1. storesDF2. persist3. StorageLevel.MEMORY_ONLY"
        ],
        "correct_answer": "E",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 153 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 149 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 129 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 145 discussion.html",
        "question": "Which of the following code blocks returns a new DataFrame that is the result of a position-wise union between DataFrame storesDF and DataFrame acquiredStoresDF?",
        "choices": [
            "A.storesDF.unionByName(acquiredStoresDF)",
            "B.unionAll(storesDF, acquiredStoresDF)",
            "C.union(storesDF, acquiredStoresDF)",
            "D.concat(storesDF, acquiredStoresDF)",
            "E.storesDF.union(acquiredStoresDF)"
        ],
        "correct_answer": "E",
        "user_data": []
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 147 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 50 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 132 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 124 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 181 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 141 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 67 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 131 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 106 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 36 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 34 discussion.html",
        "question": "Which of the following operations returns a GroupedData object?",
        "choices": [
            "A.DataFrame.GroupBy()",
            "B.DataFrame.cubed()",
            "C.DataFrame.group()",
            "D.DataFrame.groupBy()",
            "E.DataFrame.grouping_id()"
        ],
        "correct_answer": "D",
        "user_data": [
            {
                "voted_answers": "D",
                "vote_count": 2,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 81 discussion.html",
        "question": "Which of the following code blocks applies the function assessPerformance() to each row of DataFrame storesDF?",
        "choices": [
            "A.storesDF.collect.foreach(assessPerformance(row))",
            "B.storesDF.collect().apply(assessPerformance)",
            "C.storesDF.collect.apply(row => assessPerformance(row))",
            "D.storesDF.collect.map(assessPerformance(row))",
            "E.storesDF.collect.foreach(row => assessPerformance(row))"
        ],
        "correct_answer": "E",
        "user_data": [
            {
                "voted_answers": "E",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 29 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 64 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 98 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 164 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 158 discussion.html",
        "question": null,
        "choices": [],
        "correct_answer": null,
        "user_data": null
    },
    {
        "filename": "result_Databricks Certified Associate Developer for Apache Spark examtopics question 28 discussion.html",
        "question": "Which of the following code blocks returns a new DataFrame where column division from DataFrame storesDF has been replaced and renamed to column state and column managerName from DataFrame storesDF has been replaced and renamed to column managerFullName?",
        "choices": [
            "A.(storesDF.withColumnRenamed([\"division\", \"state\"], [\"managerName\", \"managerFullName\"])",
            "B.(storesDF.withColumn(\"state\", col(\"division\")).withColumn(\"managerFullName\", col(\"managerName\")))",
            "C.(storesDF.withColumn(\"state\", \"division\").withColumn(\"managerFullName\", \"managerName\"))",
            "D.(storesDF.withColumnRenamed(\"state\", \"division\").withColumnRenamed(\"managerFullName\", \"managerName\"))",
            "E.(storesDF.withColumnRenamed(\"division\", \"state\").withColumnRenamed(\"managerName\", \"managerFullName\"))"
        ],
        "correct_answer": "E",
        "user_data": [
            {
                "voted_answers": "E",
                "vote_count": 9,
                "is_most_voted": true
            },
            {
                "voted_answers": "B",
                "vote_count": 2,
                "is_most_voted": false
            },
            {
                "voted_answers": "D",
                "vote_count": 1,
                "is_most_voted": false
            }
        ]
    }
]